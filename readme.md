* 监督学习和无监督学习：监督学习中每个样本都有相应的“label”，根据这些样本作出预测，就像预测房价是回归问题，是否会得肿瘤是分类问题。回归问题推出一个连续得输出，分类问题推出一组离散得结果。无监督学习，聚类算法，一个应用谷歌新闻，收集新闻，将这些新闻分组，组成有关联的新闻。谷歌新闻就是搜索新闻事件，自动聚类，全是同一主题的显示一起。
* 单变量的线性回归：结合梯度下降运用到代价函数（建模误差平方和）。梯度下降是一个用来求函数最小值的算法，这里使用梯度下降来求代价函数的最小值。梯度下降思想：随机选择一个参数组合，计算代价函数，然后寻找下一个能让代价函数下降最多的参数组合，持续直到直到一个局部最小值。但不能确定得到的局部最小值是否是全局最小，选择不同的出事参数组合可能会找到不同的局部最小值。要注意的是参数更新是同步的。学习速率的大小是决定沿着能让代价函数下降程度最大方向迈出的步子有多大。同时要知道为什么学习速率不变也能收敛到局部最低点：当已经在一个局部的最优处或局部最优点时，局部最优点的导数为0，参数不会再更新，始终不变。且，当接近局部最低点时，梯度下降发会自动采取更小的幅度：斜率在变小，导数值变得越来越小。梯度下降用来最小化任何代价函数，不只是线性回归。
* 多变量线性回归：面对多维特征，保证特征具有相似尺度，帮助梯度下降算法更快收敛。
* 逻辑回归：与线性回归不同，对于分类问题，用线性回归来解决的化，函数的输出值可能远大于1，或远小于0，即使标签为0或1。逻辑回归是分类算法，适用于标签离散的情况。简化后的逻辑回归的梯度下降公式与线性回归的一致，但h（假设函数）不同，逻辑回归是sigmod函数，同理，也需要特征缩放。
