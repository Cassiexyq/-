* 监督学习和无监督学习：监督学习中每个样本都有相应的“label”，根据这些样本作出预测，就像预测房价是回归问题，是否会得肿瘤是分类问题。回归问题推出一个连续得输出，分类问题推出一组离散得结果。无监督学习，聚类算法，一个应用谷歌新闻，收集新闻，将这些新闻分组，组成有关联的新闻。谷歌新闻就是搜索新闻事件，自动聚类，全是同一主题的显示一起。
* 单变量的线性回归：结合梯度下降运用到代价函数（建模误差平方和）。梯度下降是一个用来求函数最小值的算法，这里使用梯度下降来求代价函数的最小值。梯度下降思想：随机选择一个参数组合，计算代价函数，然后寻找下一个能让代价函数下降最多的参数组合，持续直到直到一个局部最小值。但不能确定得到的局部最小值是否是全局最小，选择不同的出事参数组合可能会找到不同的局部最小值。要注意的是参数更新是同步的。学习速率的大小是决定沿着能让代价函数下降程度最大方向迈出的步子有多大。同时要知道为什么学习速率不变也能收敛到局部最低点：当已经在一个局部的最优处或局部最优点时，局部最优点的导数为0，参数不会再更新，始终不变。且，当接近局部最低点时，梯度下降发会自动采取更小的幅度：斜率在变小，导数值变得越来越小。梯度下降用来最小化任何代价函数，不只是线性回归。
* 多变量线性回归：面对多维特征，保证特征具有相似尺度，帮助梯度下降算法更快收敛。
* 逻辑回归：与线性回归不同，对于分类问题，用线性回归来解决的化，函数的输出值可能远大于1，或远小于0，即使标签为0或1。逻辑回归是分类算法，适用于标签离散的情况。简化后的逻辑回归的梯度下降公式与线性回归的一致，但h（假设函数）不同，逻辑回归是sigmod函数，同理，也需要特征缩放。
* 卷积神经网络在图像分类上的求解思路：一种有效的网络训练技巧微调，没必要从头开始一个一个构造深度网络，需要做的是选择一个合适的网络结构，且选择一个已经工人的且预先训练好参数权重的网络。如果只有数千张的训练样本，深度网络的参数非常多，意味着训练图像的数量要远远小于参数搜索的空间，如果只是随机初始化深度网络然后用数千张图像训练，非常容易产生“过拟合”现象。首先微调最后一层Softmax分类器，假设原来网络为1000类物体，现在只有10个类别标签，因此最后一层输出层的神经元个数变为10，使用很小的学习率来学习前连接层和分类层之间的权重矩阵，前面权重均为固定。而不断扩大微调范围，事实上，位置靠前的卷积层提取出来的特征更加的底层和具有通用性，位置靠后的卷积层以及全连接层更加与数据集的相关性更大一些，一般不会微调前几个卷积层。
* 在kaggle中的一些小trick：同一个模型，平均多个测试样例，将测试图片使用数据增强方式生成多张送进网络预测，取投票数高的作为最终结果；交叉验证训练k个模型，打乱图片顺序，把所有图片均分k次，取一份作为验证，剩余为训练，总共训练k次，把每张测试图片送入这k个模型预测，投票数最高的作为最终结果。
* 决策树：信息增益（ID3）、信息增益率（C4.5）、基尼指数（CART）。ID3算法的核心思想是以信息增益度量属性选择分裂后信息增益最大的属性进行分裂。
* 分类树与回归树：分类和回归的本质都是一样的，都是特征到结果/标签之间的映射。XGboost是由很多CART树集成。
* 集成学习：构建多个分类器对数据集进行预测，然后用某种策略将多个分类器预测的结果集成起来，作为最终预测结果。Boosting要求各分类器之间有依赖关系，必须串行，比如Adaboost、GBDT、XGBoost。Bagging各分类器之间没有依赖关系，可各自并行，比如RF。
* Adaboost：自适应增强，前一个分类器分错的样本会得到加强，加权的全体样本再次被用来训练下一个基本分类器，同时，在每一轮加入一个新的弱分类器，直到某个预定的足够小的错误率或达到预先指定的最大迭代次数。迭代算法：第一步，初始化训练数据的权值分布，如果有N个样本，每个样本最开始都被赋予相同的权值；第二步，训练弱分类器，如果某个样本点已经被准确分类，那么构造下一个训练集中权值降低，相反提高，权值更新过的样本集被用于下一个分类器，如此迭代；第三步，将各个训练得到的弱分类器组合，误差率小的弱分类器在最终的分类器中占的权重较大。
* GBDT：所有弱分类器的结果相加等于预测值，然后下一个弱分类器去拟合误差函数对预测值的残差，弱分类器的表现形式为树。
